{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "# Supervised Learning - SVC Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: To train, and use hyperparameters in supervise learning. This project illustrate how data can be classified. It technically highlighs the effects/benefits of Normalization/Standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading, Feature Selection, Cleaning, preprocessing to construct an input from Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    " #to convert scientific output to float\n",
    "np.set_printoptions(precision=20,suppress= False, formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    " #to remove warning\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data.csv data file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data.csv to pandas dataframe\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print out some sample rows and inspect the data sample to identify the dependent variables and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing out first 5 rows and all columns for inspection\n",
    "df.head()\n",
    "\n",
    "#The dependent variable is \"diagnosis\" and others columns are independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape  #This helps me see the shape of the dataframe/table in rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the values of each attribute and select a set of attributes that would only affect to predict dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "#'id' and 'Unnamed: 32' type is String will not help in predicting the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will drop 'id' and 'unnamed: 32', they have no use for machine learning\n",
    "\n",
    "del df[\"Unnamed: 32\"]\n",
    "del df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split data between dependent (Y) and independent variables (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y : dependent col ('diagnosis')\n",
    "# X: independent col (the remaining col)\n",
    "\n",
    "X, y = df.iloc[:, 1:].values, df.iloc[:, 0].values\n",
    "#[:, 1:] : all rows then the second col to the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine the data type (Discrete, Continuous, Categorical) of each attribute & Encode the dependent variable (as it is a string datatype) using a proper encoding scheme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                   object\n",
       "radius_mean                float64\n",
       "texture_mean               float64\n",
       "perimeter_mean             float64\n",
       "area_mean                  float64\n",
       "smoothness_mean            float64\n",
       "compactness_mean           float64\n",
       "concavity_mean             float64\n",
       "concave points_mean        float64\n",
       "symmetry_mean              float64\n",
       "fractal_dimension_mean     float64\n",
       "radius_se                  float64\n",
       "texture_se                 float64\n",
       "perimeter_se               float64\n",
       "area_se                    float64\n",
       "smoothness_se              float64\n",
       "compactness_se             float64\n",
       "concavity_se               float64\n",
       "concave points_se          float64\n",
       "symmetry_se                float64\n",
       "fractal_dimension_se       float64\n",
       "radius_worst               float64\n",
       "texture_worst              float64\n",
       "perimeter_worst            float64\n",
       "area_worst                 float64\n",
       "smoothness_worst           float64\n",
       "compactness_worst          float64\n",
       "concavity_worst            float64\n",
       "concave points_worst       float64\n",
       "symmetry_worst             float64\n",
       "fractal_dimension_worst    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "\n",
    "#The dependent variable (diagnosis type) is int64 and is Categorical\n",
    "#the x variables are Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='diagnosis', ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['diagnosis'], label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're converting our text categorical col (M or B) into numerical column (0,1)\n",
    "#Encode label category\n",
    "#M -> 1\n",
    "#B -> 0\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "\n",
    "y = target_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scaling, Modeling and Hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split the data into train and test data, with a ratio of 70% train and 30% test.\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n"
     ]
    }
   ],
   "source": [
    "# We're splitting our data in to Train and Test part with 70% (X_train  and y_train ) 30% (X_test and y_test) test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Demonstrate results before and after using Standard Scaler. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you fit any preprocessing algorithms on the whole dataset? No! We do not fit preprocessing algorithms on the whole dataset.\n",
    "\n",
    "Why? \n",
    "During preprocessing it is important to use only the data relevant for machine learning for fitting. Fitting finds the internal parameters of a model that will be used to transform data. Those parameters to map new or existing values of x to the dependent variable, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9181286549707602"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting BEFORE Standard Scaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "#accuracy\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n"
     ]
    }
   ],
   "source": [
    "#Transformation with Standard Scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Initializing the scaler function\n",
    "scaler = StandardScaler()\n",
    "X, y = df.iloc[:, 1:].values, df.iloc[:, 0].values\n",
    "\n",
    "#Fit_transform\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "target_encoder = LabelEncoder()\n",
    "\n",
    "y = target_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting AFTER Standard Scaler\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train,y_train)\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Also output classifier train and test accuracy scores and confusion matrix for each result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy Scores before Standardizing\n",
    "#LinearSVC accuracy scores\n",
    "\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "classifier = LinearSVC()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_predict = classifier.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   2]\n",
      " [  4  61]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix before Standardizing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC accuracy scores before standardizing\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train,y_train)# fitting data using SVC()\n",
    "classifier.score(X_test,y_test)# #accuracy of svc() model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy Scores AFTER Standardizing\n",
    "#LinearSVC accuracy scores\n",
    "\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "classifier = LinearSVC()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_predict = classifier.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   2]\n",
      " [  4  61]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix AFTER Standardizing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC accuracy scores AFTER standardizing\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train,y_train)\n",
    "classifier.score(X_test,y_test)\n",
    "\n",
    "#There is an improvement in accuracy after using Standard Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   9. Finally perform Hyper-parameter tuning using GridSearchCV or RandomSearchCV, and note down the result train and test accuracies as well as the best parameters.\n",
    "       Write Observations on benefits/disadvantages of Standardization/Normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001000, 0.011788, 0.138950, 1.637894, 19.306977, 227.584593,\n",
       "       2682.695795, 31622.776602, 372759.372031, 4393970.560761,\n",
       "       51794746.792312, 610540229.658531, 7196856730.011529,\n",
       "       84834289824.407257, 1000000000000.000000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-3, 12, 15)\n",
    "# Return numbers spaced evenly on a log scale. we are generation 15 numbers from -3 to 12 which are evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000001,\n",
       "       0.000019, 0.000316, 0.005179, 0.084834, 1.389495, 22.758459,\n",
       "       372.759372, 6105.402297, 100000.000000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-12, 5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([0.001000, 0.011788, 0.138950, 1.637894, 19.306977, 227.584593,\n",
       "       2682.695795, 31622.776602, 372759.372031, 4393970.560761,\n",
       "       51794746.792312, 610540229.658531, 7196856730.011529,\n",
       "       84834289824.407257, 1000000000000.000000]),\n",
       "                         'gamma': array([0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000001,\n",
       "       0.000019, 0.000316, 0.005179, 0.084834, 1.389495, 22.758459,\n",
       "       372.759372, 6105.402297, 100000.000000])})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_range = np.logspace(-3, 12, 15)\n",
    "gamma_range = np.logspace(-12, 5, 15)\n",
    "#We are just creaing a dictionary\n",
    "param_grid = dict( gamma=gamma_range,C=C_range)\n",
    "#We've got a total of 15+15 = 30 hyperparameters, which make a total of 15*15=225 as our hyperparameter search space\n",
    "#Using grid search we're searching for the model with the best score in this search space that has 225 combinations\n",
    " #Refit an estimator using the best found parameters on the whole dataset\n",
    "grid = GridSearchCV(SVC(),refit = True, param_grid=param_grid)\n",
    "#we fitting it to our data and searching for models of all combinations (169 combination)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1e-12}</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1.637893706954061e-11}</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'C': 0.001, 'gamma': 2.6826957952797273e-10}</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'C': 0.001, 'gamma': 4.393970560760786e-09}</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005199</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'C': 0.001, 'gamma': 7.196856730011513e-08}</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>1000000000000.0</td>\n",
       "      <td>1.389495</td>\n",
       "      <td>{'C': 1000000000000.0, 'gamma': 1.389495494373...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>1000000000000.0</td>\n",
       "      <td>22.758459</td>\n",
       "      <td>{'C': 1000000000000.0, 'gamma': 22.75845926074...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.006992</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>1000000000000.0</td>\n",
       "      <td>372.759372</td>\n",
       "      <td>{'C': 1000000000000.0, 'gamma': 372.7593720314...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>1000000000000.0</td>\n",
       "      <td>6105.402297</td>\n",
       "      <td>{'C': 1000000000000.0, 'gamma': 6105.402296585...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.006197</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1000000000000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>{'C': 1000000000000.0, 'gamma': 100000.0}</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.005413      0.000486         0.003394        0.000491   \n",
       "1         0.004591      0.000477         0.003604        0.000494   \n",
       "2         0.005202      0.000399         0.003800        0.000400   \n",
       "3         0.005598      0.000489         0.003802        0.000398   \n",
       "4         0.005199      0.000400         0.003809        0.000405   \n",
       "..             ...           ...              ...             ...   \n",
       "220       0.004995      0.000011         0.002201        0.000401   \n",
       "221       0.006597      0.000491         0.002810        0.000406   \n",
       "222       0.006992      0.000891         0.003805        0.000763   \n",
       "223       0.012198      0.000738         0.005196        0.001718   \n",
       "224       0.006197      0.000402         0.003007        0.000019   \n",
       "\n",
       "             param_C  param_gamma  \\\n",
       "0              0.001          0.0   \n",
       "1              0.001          0.0   \n",
       "2              0.001          0.0   \n",
       "3              0.001          0.0   \n",
       "4              0.001          0.0   \n",
       "..               ...          ...   \n",
       "220  1000000000000.0     1.389495   \n",
       "221  1000000000000.0    22.758459   \n",
       "222  1000000000000.0   372.759372   \n",
       "223  1000000000000.0  6105.402297   \n",
       "224  1000000000000.0     100000.0   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "0                         {'C': 0.001, 'gamma': 1e-12}              0.625   \n",
       "1         {'C': 0.001, 'gamma': 1.637893706954061e-11}              0.625   \n",
       "2        {'C': 0.001, 'gamma': 2.6826957952797273e-10}              0.625   \n",
       "3         {'C': 0.001, 'gamma': 4.393970560760786e-09}              0.625   \n",
       "4         {'C': 0.001, 'gamma': 7.196856730011513e-08}              0.625   \n",
       "..                                                 ...                ...   \n",
       "220  {'C': 1000000000000.0, 'gamma': 1.389495494373...              0.625   \n",
       "221  {'C': 1000000000000.0, 'gamma': 22.75845926074...              0.625   \n",
       "222  {'C': 1000000000000.0, 'gamma': 372.7593720314...              0.625   \n",
       "223  {'C': 1000000000000.0, 'gamma': 6105.402296585...              0.625   \n",
       "224          {'C': 1000000000000.0, 'gamma': 100000.0}              0.625   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0                0.625              0.625           0.632911   \n",
       "1                0.625              0.625           0.632911   \n",
       "2                0.625              0.625           0.632911   \n",
       "3                0.625              0.625           0.632911   \n",
       "4                0.625              0.625           0.632911   \n",
       "..                 ...                ...                ...   \n",
       "220              0.625              0.625           0.632911   \n",
       "221              0.625              0.625           0.632911   \n",
       "222              0.625              0.625           0.632911   \n",
       "223              0.625              0.625           0.632911   \n",
       "224              0.625              0.625           0.632911   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0             0.620253         0.625633        0.004077               96  \n",
       "1             0.620253         0.625633        0.004077               96  \n",
       "2             0.620253         0.625633        0.004077               96  \n",
       "3             0.620253         0.625633        0.004077               96  \n",
       "4             0.620253         0.625633        0.004077               96  \n",
       "..                 ...              ...             ...              ...  \n",
       "220           0.620253         0.625633        0.004077               96  \n",
       "221           0.620253         0.625633        0.004077               96  \n",
       "222           0.620253         0.625633        0.004077               96  \n",
       "223           0.620253         0.625633        0.004077               96  \n",
       "224           0.620253         0.625633        0.004077               96  \n",
       "\n",
       "[225 rows x 15 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 31622.776601683792, 'gamma': 1.9306977288832455e-05} with a score of 0.98\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations on benefits/disadvantages of Standardization/Normalization\n",
    "\n",
    "Standardization is great to use as our data follows a normal distribution which is clearly understandable from the df.describe() table. Our data is not influenced by the maximum or minimum value which does not have a bounding range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
